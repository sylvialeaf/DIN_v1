#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®éªŒä¸‰ï¼šDIN æ”¹è¿›æ¶ˆèå®éªŒ

åœ¨ä¸°å¯Œç‰¹å¾åŸºç¡€ä¸Šï¼Œæµ‹è¯•ä¸åŒæ”¹è¿›ç­–ç•¥çš„æ•ˆæœã€‚

æ¶ˆèå˜ä½“ï¼ˆä¸READMEå¯¹é½ï¼‰ï¼š
1. DIN-Base: åŸºç¡€DINï¼ˆæ— æ”¹è¿›ï¼‰
2. DIN-TimeDec: + æ—¶é—´è¡°å‡æ³¨æ„åŠ›
3. DIN-MultiHead: + å¤šå¤´æ³¨æ„åŠ›
4. DIN-Enhanced: + å¢å¼ºMLPï¼ˆæ›´æ·±çš„ç½‘ç»œï¼‰
5. DIN-Full: æ—¶é—´è¡°å‡ + å¢å¼ºMLPï¼ˆæœ€ä½³ç»„åˆï¼‰

è¯„ä¼°æŒ‡æ ‡ï¼š
- CTR: AUC, LogLoss
- Top-K: HR@K, NDCG@K, MRR@Kï¼ˆä¸å®éªŒ1/2ç»Ÿä¸€ï¼‰

è¾“å‡º:
- results/experiment3_results.csv
- results/experiment3_plot.png
- results/experiment3_report.json
"""

import os
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import json
import time

from data_loader import get_rich_dataloaders, get_topk_eval_data
from trainer import RichTrainer, measure_inference_speed_rich
from feature_engineering import InteractionFeatureExtractor


# ========================================
# æ”¹è¿›ç‰ˆæ³¨æ„åŠ›å±‚ï¼ˆæ”¯æŒä¸°å¯Œç‰¹å¾ï¼‰
# ========================================

class TimeDecayRichAttention(nn.Module):
    """
    æ—¶é—´è¡°å‡ + ä¸°å¯Œç‰¹å¾æ³¨æ„åŠ›
    
    è¿‘æœŸè¡Œä¸ºæƒé‡æ›´é«˜ï¼Œç¬¦åˆå…´è¶£æ¼‚ç§»è§„å¾‹ã€‚
    """
    
    def __init__(self, input_dim, hidden_dims=[64, 32], time_decay=0.1):
        super(TimeDecayRichAttention, self).__init__()
        
        self.time_decay = time_decay
        
        mlp_input = 4 * input_dim
        layers = []
        prev_dim = mlp_input
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.PReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        
        self.attention_mlp = nn.Sequential(*layers)
    
    def forward(self, query, keys, keys_mask=None):
        batch_size, seq_len, dim = keys.shape
        
        query_expanded = query.unsqueeze(1).expand(-1, seq_len, -1)
        
        attention_input = torch.cat([
            keys, query_expanded,
            keys * query_expanded,
            keys - query_expanded
        ], dim=-1)
        
        attention_scores = self.attention_mlp(attention_input).squeeze(-1)
        
        # æ—¶é—´è¡°å‡ï¼šä½ç½®è¶Šé åï¼ˆè¶Šè¿‘ï¼‰ï¼Œæƒé‡è¶Šå¤§
        positions = torch.arange(seq_len, device=keys.device).float()
        time_weights = torch.exp(self.time_decay * (positions - seq_len + 1))
        attention_scores = attention_scores * time_weights.unsqueeze(0)
        
        if keys_mask is not None:
            attention_scores = attention_scores.masked_fill(~keys_mask.bool(), -1e9)
        
        attention_weights = F.softmax(attention_scores, dim=-1)
        weighted_sum = torch.sum(attention_weights.unsqueeze(-1) * keys, dim=1)
        
        return weighted_sum, attention_weights


class MultiHeadRichAttention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ› + ä¸°å¯Œç‰¹å¾
    
    æ•è·ç”¨æˆ·çš„å¤šç»´å…´è¶£ã€‚
    """
    
    def __init__(self, input_dim, num_heads=4, hidden_dims=[64, 32]):
        super(MultiHeadRichAttention, self).__init__()
        
        self.num_heads = num_heads
        
        self.attention_heads = nn.ModuleList([
            self._build_attention_mlp(4 * input_dim, hidden_dims)
            for _ in range(num_heads)
        ])
        
        self.output_proj = nn.Linear(input_dim, input_dim)
    
    def _build_attention_mlp(self, input_dim, hidden_dims):
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.PReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        return nn.Sequential(*layers)
    
    def forward(self, query, keys, keys_mask=None):
        batch_size, seq_len, dim = keys.shape
        
        query_expanded = query.unsqueeze(1).expand(-1, seq_len, -1)
        
        attention_input = torch.cat([
            keys, query_expanded,
            keys * query_expanded,
            keys - query_expanded
        ], dim=-1)
        
        head_outputs = []
        for head in self.attention_heads:
            scores = head(attention_input).squeeze(-1)
            
            if keys_mask is not None:
                scores = scores.masked_fill(~keys_mask.bool(), -1e9)
            
            weights = F.softmax(scores, dim=-1)
            output = torch.sum(weights.unsqueeze(-1) * keys, dim=1)
            head_outputs.append(output)
        
        combined = torch.stack(head_outputs, dim=1).mean(dim=1)
        output = self.output_proj(combined)
        
        return output, None


# ========================================
# æ”¹è¿›ç‰ˆ DIN æ¨¡å‹
# ========================================

class DINRichImproved(nn.Module):
    """
    æ”¹è¿›ç‰ˆä¸°å¯Œç‰¹å¾ DIN
    
    æ”¯æŒä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“å’ŒMLPé…ç½®ã€‚
    
    å˜ä½“è¯´æ˜ï¼š
    - base: åŸºç¡€æ³¨æ„åŠ› + æ ‡å‡†MLP
    - time_decay: æ—¶é—´è¡°å‡æ³¨æ„åŠ› + æ ‡å‡†MLP
    - multi_head: å¤šå¤´æ³¨æ„åŠ› + æ ‡å‡†MLP
    - enhanced: åŸºç¡€æ³¨æ„åŠ› + å¢å¼ºMLPï¼ˆæ›´æ·±æ›´å®½ï¼‰
    - full: æ—¶é—´è¡°å‡æ³¨æ„åŠ› + å¢å¼ºMLPï¼ˆæœ€ä½³ç»„åˆï¼‰
    """
    
    def __init__(
        self,
        num_items,
        num_users,
        feature_dims,
        embedding_dim=64,
        feature_embedding_dim=16,
        attention_type='base',  # 'base', 'time_decay', 'multi_head'
        use_enhanced_mlp=False,  # æ˜¯å¦ä½¿ç”¨å¢å¼ºMLP
        mlp_hidden_dims=[256, 128, 64],
        dropout_rate=0.2,
        num_heads=4,
        time_decay=0.1
    ):
        super(DINRichImproved, self).__init__()
        
        self.embedding_dim = embedding_dim
        self.feature_embedding_dim = feature_embedding_dim
        self.attention_type = attention_type
        self.use_enhanced_mlp = use_enhanced_mlp
        
        # åµŒå…¥å±‚
        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)
        self.user_embedding = nn.Embedding(num_users + 1, feature_embedding_dim)
        self.genre_embedding = nn.Embedding(feature_dims.get('primary_genre', 20) + 1, feature_embedding_dim, padding_idx=0)
        self.year_embedding = nn.Embedding(feature_dims.get('year_bucket', 8) + 1, feature_embedding_dim, padding_idx=0)
        self.age_embedding = nn.Embedding(feature_dims.get('age_bucket', 10) + 1, feature_embedding_dim)
        self.gender_embedding = nn.Embedding(3, feature_embedding_dim)
        self.occupation_embedding = nn.Embedding(feature_dims.get('occupation', 25) + 1, feature_embedding_dim)
        
        # åºåˆ—ç‰¹å¾ç»´åº¦
        self.seq_feature_dim = embedding_dim + 2 * feature_embedding_dim
        
        # é€‰æ‹©æ³¨æ„åŠ›æœºåˆ¶
        if attention_type == 'base':
            from models import AttentionLayer
            self.attention = AttentionLayer(self.seq_feature_dim, [64, 32])
        elif attention_type == 'time_decay':
            self.attention = TimeDecayRichAttention(self.seq_feature_dim, [64, 32], time_decay)
        elif attention_type == 'multi_head':
            self.attention = MultiHeadRichAttention(self.seq_feature_dim, num_heads, [64, 32])
        else:
            raise ValueError(f"Unknown attention type: {attention_type}")
        
        # MLPè¾“å…¥ç»´åº¦
        mlp_input_dim = (
            self.seq_feature_dim +  # ç”¨æˆ·å…´è¶£
            self.seq_feature_dim +  # ç›®æ ‡ç‰©å“
            feature_embedding_dim +  # ç”¨æˆ·
            feature_embedding_dim * 3  # å¹´é¾„ + æ€§åˆ« + èŒä¸š
        )
        
        # é€‰æ‹©MLPé…ç½®
        if use_enhanced_mlp:
            # å¢å¼ºMLPï¼šæ›´æ·±æ›´å®½ï¼Œå¸¦æ®‹å·®è¿æ¥
            self.mlp = self._build_enhanced_mlp(mlp_input_dim, dropout_rate)
        else:
            # æ ‡å‡†MLP
            self.mlp = self._build_standard_mlp(mlp_input_dim, mlp_hidden_dims, dropout_rate)
        
        self._init_weights()
    
    def _build_standard_mlp(self, input_dim, hidden_dims, dropout_rate):
        """æ ‡å‡†MLP"""
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.PReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        return nn.Sequential(*layers)
    
    def _build_enhanced_mlp(self, input_dim, dropout_rate):
        """å¢å¼ºMLPï¼šæ›´æ·±æ›´å®½ï¼Œå¸¦æ®‹å·®é£æ ¼çš„è·³è·ƒè¿æ¥"""
        # å¢å¼ºé…ç½®ï¼š[512, 256, 128, 64]ï¼Œæ¯”æ ‡å‡†ç‰ˆæ›´æ·±
        hidden_dims = [512, 256, 128, 64]
        
        layers = []
        prev_dim = input_dim
        
        # ç¬¬ä¸€å±‚ï¼šæŠ•å½±åˆ°512
        layers.append(nn.Linear(prev_dim, hidden_dims[0]))
        layers.append(nn.BatchNorm1d(hidden_dims[0]))
        layers.append(nn.PReLU())
        layers.append(nn.Dropout(dropout_rate))
        prev_dim = hidden_dims[0]
        
        # åç»­å±‚
        for hidden_dim in hidden_dims[1:]:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.PReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, 1))
        
        return nn.Sequential(*layers)
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.01)
    
    def forward(self, batch):
        # å†å²åºåˆ—åµŒå…¥
        seq_item_emb = self.item_embedding(batch['item_seq'])
        seq_genre_emb = self.genre_embedding(batch['history_genres'])
        seq_year_emb = self.year_embedding(batch['history_years'])
        seq_combined = torch.cat([seq_item_emb, seq_genre_emb, seq_year_emb], dim=-1)
        
        # ç›®æ ‡ç‰©å“åµŒå…¥
        target_item_emb = self.item_embedding(batch['target_item'])
        target_genre_emb = self.genre_embedding(batch['item_genre'])
        target_year_emb = self.year_embedding(batch['item_year'])
        target_combined = torch.cat([target_item_emb, target_genre_emb, target_year_emb], dim=-1)
        
        # æ³¨æ„åŠ›
        user_interest, _ = self.attention(target_combined, seq_combined, batch['item_seq_mask'])
        
        # ç”¨æˆ·ç‰¹å¾
        user_emb = self.user_embedding(batch['user_id'])
        age_emb = self.age_embedding(batch['user_age'])
        gender_emb = self.gender_embedding(batch['user_gender'])
        occupation_emb = self.occupation_embedding(batch['user_occupation'])
        
        # æ‹¼æ¥
        features = torch.cat([
            user_interest, target_combined,
            user_emb, age_emb, gender_emb, occupation_emb
        ], dim=-1)
        
        return self.mlp(features).squeeze(-1)


# ========================================
# ä¸»å®éªŒ
# ========================================

print("=" * 80)
print("å®éªŒä¸‰ï¼šDIN æ”¹è¿›æ¶ˆèå®éªŒ")
print("=" * 80)

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"è®¾å¤‡: {DEVICE}")

# å®éªŒå‚æ•°
MAX_SEQ_LENGTH = 50
EPOCHS = 20
BATCH_SIZE = 256
EMBEDDING_DIM = 64

# æ¶ˆèé…ç½®ï¼ˆä¸READMEå¯¹é½ï¼š5ä¸ªå˜ä½“ï¼‰
ABLATION_CONFIGS = [
    {
        'name': 'DIN-Base', 
        'attention_type': 'base', 
        'use_enhanced_mlp': False,
        'description': 'åŸºç¡€DINï¼ˆæ— æ”¹è¿›ï¼‰'
    },
    {
        'name': 'DIN-TimeDec', 
        'attention_type': 'time_decay', 
        'use_enhanced_mlp': False,
        'description': '+ æ—¶é—´è¡°å‡æ³¨æ„åŠ›'
    },
    {
        'name': 'DIN-MultiHead', 
        'attention_type': 'multi_head', 
        'use_enhanced_mlp': False,
        'description': '+ å¤šå¤´æ³¨æ„åŠ›'
    },
    {
        'name': 'DIN-Enhanced', 
        'attention_type': 'base', 
        'use_enhanced_mlp': True,
        'description': '+ å¢å¼ºMLPï¼ˆæ›´æ·±ç½‘ç»œï¼‰'
    },
    {
        'name': 'DIN-Full', 
        'attention_type': 'time_decay', 
        'use_enhanced_mlp': True,
        'description': 'æ—¶é—´è¡°å‡ + å¢å¼ºMLPï¼ˆæœ€ä½³ç»„åˆï¼‰'
    },
]

RESULTS_DIR = os.path.join(os.path.dirname(__file__), 'results')
os.makedirs(RESULTS_DIR, exist_ok=True)

results = []
start_time = datetime.now()

print(f"\nå¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"æ¶ˆèé…ç½®æ•°: {len(ABLATION_CONFIGS)}")
print()

# åŠ è½½æ•°æ®
print("ğŸ“¦ åŠ è½½æ•°æ®...")
train_loader, valid_loader, test_loader, dataset_info, fp = get_rich_dataloaders(
    data_dir='./data',
    dataset_name='ml-100k',
    max_seq_length=MAX_SEQ_LENGTH,
    batch_size=BATCH_SIZE
)

# Top-K è¯„ä¼°æ•°æ®ï¼ˆä¸å®éªŒ1/2ç»Ÿä¸€ï¼‰
eval_data, eval_info, fp_topk, interaction_extractor = get_topk_eval_data(
    data_dir='./data',
    dataset_name='ml-100k',
    max_seq_length=MAX_SEQ_LENGTH
)

for config in ABLATION_CONFIGS:
    print("\n" + "=" * 80)
    print(f"ğŸš€ {config['name']}: {config['description']}")
    print("=" * 80)
    
    try:
        model = DINRichImproved(
            num_items=dataset_info['num_items'],
            num_users=dataset_info['num_users'],
            feature_dims=dataset_info['feature_dims'],
            embedding_dim=EMBEDDING_DIM,
            attention_type=config['attention_type'],
            use_enhanced_mlp=config['use_enhanced_mlp'],
            mlp_hidden_dims=[256, 128, 64],
            dropout_rate=0.2
        )
        
        trainer = RichTrainer(model=model, device=DEVICE)
        
        t1 = time.time()
        train_result = trainer.fit(
            train_loader=train_loader,
            valid_loader=valid_loader,
            epochs=EPOCHS,
            early_stopping_patience=5,
            show_progress=True
        )
        train_time = time.time() - t1
        
        # CTR è¯„ä¼°
        test_metrics = trainer.evaluate(test_loader)
        speed = measure_inference_speed_rich(model, test_loader, DEVICE)
        
        # Top-K è¯„ä¼°ï¼ˆä¸å®éªŒ1/2ç»Ÿä¸€ï¼‰
        topk_metrics = trainer.evaluate_topk(
            eval_data=eval_data,
            feature_processor=fp_topk,
            interaction_extractor=interaction_extractor,
            max_seq_length=MAX_SEQ_LENGTH,
            ks=[5, 10, 20],
            show_progress=False
        )
        
        result_entry = {
            'variant': config['name'],
            'description': config['description'],
            'test_auc': test_metrics['auc'],
            'test_logloss': test_metrics['logloss'],
            'best_valid_auc': train_result['best_valid_auc'],
            'train_time_sec': train_time,
            'qps': speed['qps'],
            'num_params': sum(p.numel() for p in model.parameters()),
            'status': 'success'
        }
        result_entry.update(topk_metrics)
        results.append(result_entry)
        
        print(f"\nâœ… {config['name']} å®Œæˆ!")
        print(f"   Test AUC: {test_metrics['auc']:.4f}")
        print(f"   Test LogLoss: {test_metrics['logloss']:.4f}")
        print(f"   HR@10: {topk_metrics['HR@10']:.4f}, NDCG@10: {topk_metrics['NDCG@10']:.4f}")
        print(f"   QPS: {speed['qps']:.0f}")
        
    except Exception as e:
        print(f"âŒ {config['name']} é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
        results.append({
            'variant': config['name'],
            'description': config['description'],
            'test_auc': None,
            'test_logloss': None,
            'best_valid_auc': None,
            'train_time_sec': None,
            'qps': None,
            'status': f'error: {str(e)[:100]}'
        })

# å®Œæˆ
end_time = datetime.now()
total_time = (end_time - start_time).total_seconds()

# ä¿å­˜ç»“æœ
df_results = pd.DataFrame(results)
results_file = os.path.join(RESULTS_DIR, 'experiment3_results.csv')
df_results.to_csv(results_file, index=False)

print("\n" + "=" * 80)
print("ğŸ‰ å®éªŒä¸‰å®Œæˆ!")
print("=" * 80)

# å¯è§†åŒ–
print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
df_success = df_results[df_results['status'] == 'success'].copy()

if len(df_success) > 0:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#DDA0DD']
    
    # AUC å¯¹æ¯”
    bars = axes[0, 0].bar(
        range(len(df_success)), 
        df_success['test_auc'],
        color=colors[:len(df_success)]
    )
    axes[0, 0].set_xticks(range(len(df_success)))
    axes[0, 0].set_xticklabels(df_success['variant'], rotation=20, ha='right')
    axes[0, 0].set_ylabel('Test AUC', fontsize=12)
    axes[0, 0].set_title('æ¶ˆèå®éªŒ: AUC å¯¹æ¯”', fontsize=14, fontweight='bold')
    for bar, val in zip(bars, df_success['test_auc']):
        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
                    f'{val:.4f}', ha='center', va='bottom', fontsize=9)
    
    # æ”¹è¿›å¹…åº¦ï¼ˆç›¸å¯¹Baseï¼‰
    base_auc = df_success[df_success['variant'] == 'DIN-Base']['test_auc'].values[0]
    improvements = [(auc - base_auc) * 100 for auc in df_success['test_auc']]  # ç»å¯¹æå‡ Ã— 100
    
    bars = axes[0, 1].bar(
        range(len(df_success)), 
        improvements,
        color=colors[:len(df_success)]
    )
    axes[0, 1].set_xticks(range(len(df_success)))
    axes[0, 1].set_xticklabels(df_success['variant'], rotation=20, ha='right')
    axes[0, 1].set_ylabel('ç›¸å¯¹åŸºçº¿ AUC æå‡ (Ã—100)', fontsize=12)
    axes[0, 1].set_title('æ¶ˆèå®éªŒ: æ”¹è¿›å¹…åº¦ (Î”AUC)', fontsize=14, fontweight='bold')
    axes[0, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    for bar, val in zip(bars, improvements):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2,
                    bar.get_height() + 0.1 if val >= 0 else bar.get_height() - 0.3,
                    f'{val/100:+.4f}', ha='center', va='bottom', fontsize=9)
    
    # NDCG@10 å¯¹æ¯”ï¼ˆTop-KæŒ‡æ ‡ï¼‰
    if 'NDCG@10' in df_success.columns:
        bars = axes[1, 0].bar(
            range(len(df_success)), 
            df_success['NDCG@10'],
            color=colors[:len(df_success)]
        )
        axes[1, 0].set_xticks(range(len(df_success)))
        axes[1, 0].set_xticklabels(df_success['variant'], rotation=20, ha='right')
        axes[1, 0].set_ylabel('NDCG@10', fontsize=12)
        axes[1, 0].set_title('æ¶ˆèå®éªŒ: Top-K æ¨èè´¨é‡', fontsize=14, fontweight='bold')
        for bar, val in zip(bars, df_success['NDCG@10']):
            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                        f'{val:.4f}', ha='center', va='bottom', fontsize=9)
    
    # QPS å¯¹æ¯”
    bars = axes[1, 1].bar(
        range(len(df_success)), 
        df_success['qps'],
        color=colors[:len(df_success)]
    )
    axes[1, 1].set_xticks(range(len(df_success)))
    axes[1, 1].set_xticklabels(df_success['variant'], rotation=20, ha='right')
    axes[1, 1].set_ylabel('QPS', fontsize=12)
    axes[1, 1].set_title('æ¶ˆèå®éªŒ: æ¨ç†é€Ÿåº¦', fontsize=14, fontweight='bold')
    for bar, val in zip(bars, df_success['qps']):
        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,
                    f'{val:.0f}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plot_file = os.path.join(RESULTS_DIR, 'experiment3_plot.png')
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {plot_file}")
    plt.close()

# æŠ¥å‘Š
report = {
    'experiment': 'Experiment 3: DIN Improvement Ablation Study',
    'dataset': 'ml-100k',
    'ablation_configs': [
        {'name': c['name'], 'description': c['description']} 
        for c in ABLATION_CONFIGS
    ],
    'ablation_factors': {
        'time_decay_attention': 'æ—¶é—´è¡°å‡æ³¨æ„åŠ›ï¼šè¿‘æœŸè¡Œä¸ºæƒé‡æ›´é«˜',
        'multi_head_attention': 'å¤šå¤´æ³¨æ„åŠ›ï¼šæ•è·å¤šç»´å…´è¶£',
        'enhanced_mlp': 'å¢å¼ºMLPï¼šæ›´æ·±ç½‘ç»œ [512, 256, 128, 64]'
    },
    'features_used': [
        'item_id', 'user_id',
        'history_genres', 'history_years',
        'item_genre', 'item_year',
        'user_age', 'user_gender', 'user_occupation'
    ],
    'total_time_seconds': total_time,
    'results': results
}

if len(df_success) > 0:
    best_idx = df_success['test_auc'].idxmax()
    report['best_variant'] = df_success.loc[best_idx, 'variant']
    report['best_auc'] = float(df_success.loc[best_idx, 'test_auc'])
    report['baseline_auc'] = float(base_auc)
    report['improvements'] = {
        row['variant']: float(row['test_auc'] - base_auc)
        for _, row in df_success.iterrows()
    }

report_file = os.path.join(RESULTS_DIR, 'experiment3_report.json')
with open(report_file, 'w', encoding='utf-8') as f:
    json.dump(report, f, indent=2, ensure_ascii=False)

# æ‰“å°ç»“æœ
print("\n" + "=" * 80)
print("ğŸ“‹ å®éªŒç»“æœæ‘˜è¦")
print("=" * 80)

# CTR æŒ‡æ ‡
print("\nğŸ“Š CTR æŒ‡æ ‡:")
print(df_results[['variant', 'test_auc', 'test_logloss', 'qps', 'num_params']].to_string(index=False))

# Top-K æŒ‡æ ‡
if 'HR@10' in df_results.columns:
    print("\nğŸ“Š Top-K æ¨èæŒ‡æ ‡:")
    topk_cols = ['variant', 'HR@10', 'NDCG@10', 'MRR@10']
    print(df_results[topk_cols].to_string(index=False))

if len(df_success) > 0:
    print("\nğŸ” å…³é”®å‘ç°:")
    print(f"   åŸºçº¿ AUC: {base_auc:.4f}")
    print(f"   æœ€ä½³å˜ä½“: {report.get('best_variant', 'N/A')} (AUC={report.get('best_auc', 0):.4f})")
    
    for _, row in df_success.iterrows():
        delta = row['test_auc'] - base_auc
        print(f"   {row['variant']}: AUC={row['test_auc']:.4f} (Î”AUC={delta:+.4f})")

print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
print(f"   - {results_file}")
print(f"   - {os.path.join(RESULTS_DIR, 'experiment3_plot.png')}")
print(f"   - {report_file}")
print("=" * 80)
