#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
äº‘ç«¯ GPU å®Œæ•´å®éªŒè„šæœ¬

é€‚åˆåœ¨ AutoDL / Colab / é˜¿é‡Œäº‘ç­‰ GPU ç¯å¢ƒè¿è¡Œã€‚
æ”¯æŒ ml-100k å’Œ ml-1m åŒæ•°æ®é›†ã€‚
åŒ…å«å…¨éƒ¨ä¸‰ä¸ªå®éªŒã€‚

ä½¿ç”¨æ–¹æ³•:
    python run_all_gpu.py                    # è¿è¡Œæ‰€æœ‰å®éªŒï¼ˆä¸¤ä¸ªæ•°æ®é›†ï¼‰
    python run_all_gpu.py --dataset ml-100k  # åªè¿è¡Œ ml-100k
    python run_all_gpu.py --dataset ml-1m    # åªè¿è¡Œ ml-1m
    python run_all_gpu.py --quick            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    
    # å•ç‹¬è¿è¡ŒæŸä¸ªå®éªŒ
    python run_all_gpu.py --exp 1            # åªè¿è¡Œå®éªŒ1
    python run_all_gpu.py --exp 2            # åªè¿è¡Œå®éªŒ2
    python run_all_gpu.py --exp 3            # åªè¿è¡Œå®éªŒ3
    python run_all_gpu.py --exp 1,2          # è¿è¡Œå®éªŒ1å’Œ2

é¢„ä¼°æ—¶é—´ (GPU, ä¸¤ä¸ªæ•°æ®é›†):
    å®éªŒ1ï¼ˆåºåˆ—é•¿åº¦+æ¨¡å‹å¯¹æ¯”ï¼‰: çº¦ 40-60 åˆ†é’Ÿ
    å®éªŒ2ï¼ˆæ–¹æ³•å¯¹æ¯”+æ··åˆç²¾æ’ï¼‰: çº¦ 30-40 åˆ†é’Ÿ
    å®éªŒ3ï¼ˆæ¶ˆèå®éªŒï¼‰:          çº¦ 20-30 åˆ†é’Ÿ
    æ€»è®¡:                       çº¦ 1.5-2.5 å°æ—¶
"""

import os
import sys
import argparse
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import json
import time
from tqdm import tqdm

from data_loader import get_rich_dataloaders
from models import DINRichLite, SimpleAveragePoolingRich, GRU4Rec, SASRec, NARM, AttentionLayer
from trainer import RichTrainer, measure_inference_speed_rich
from feature_engineering import FeatureProcessor, InteractionFeatureExtractor, prepare_lightgbm_features

try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False
    print("âš ï¸ LightGBM æœªå®‰è£…ï¼Œæ··åˆç²¾æ’å°†è·³è¿‡")

# ========================================
# é…ç½®
# ========================================

parser = argparse.ArgumentParser(description='äº‘ç«¯ GPU å®Œæ•´å®éªŒ')
parser.add_argument('--dataset', type=str, default='both', 
                    choices=['ml-100k', 'ml-1m', 'both'],
                    help='æ•°æ®é›†é€‰æ‹©')
parser.add_argument('--quick', action='store_true', 
                    help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå‡å°‘ epochs å’Œåºåˆ—é•¿åº¦ï¼‰')
parser.add_argument('--epochs', type=int, default=50,
                    help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 50ï¼‰')
parser.add_argument('--exp', type=str, default='all',
                    help='è¿è¡Œå“ªäº›å®éªŒ: 1, 2, 3, 1,2, 1,3, 2,3, all')
args = parser.parse_args()

# è§£æè¦è¿è¡Œçš„å®éªŒ
if args.exp == 'all':
    EXPERIMENTS_TO_RUN = [1, 2, 3]
else:
    EXPERIMENTS_TO_RUN = [int(x.strip()) for x in args.exp.split(',')]

# è®¾å¤‡æ£€æµ‹
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# å®éªŒå‚æ•°
if args.quick:
    EPOCHS = 10
    SEQ_LENGTHS = [20, 50]
    BATCH_SIZE = 512
else:
    EPOCHS = args.epochs
    SEQ_LENGTHS = [20, 50, 100, 150]
    BATCH_SIZE = 512 if DEVICE == 'cuda' else 256

EMBEDDING_DIM = 64
MODELS_TO_TEST = ['DIN', 'GRU4Rec', 'SASRec', 'NARM', 'AvgPool']

# æ•°æ®é›†
if args.dataset == 'both':
    DATASETS = ['ml-100k', 'ml-1m']
else:
    DATASETS = [args.dataset]

# ç»“æœç›®å½•
RESULTS_DIR = os.path.join(os.path.dirname(__file__), 'results_gpu')
os.makedirs(RESULTS_DIR, exist_ok=True)

print("=" * 80)
print("ğŸš€ äº‘ç«¯ GPU å®Œæ•´å®éªŒ")
print("=" * 80)
print(f"è®¾å¤‡: {DEVICE}")
if DEVICE == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"æ•°æ®é›†: {DATASETS}")
print(f"å®éªŒ: {EXPERIMENTS_TO_RUN}")
print(f"Epochs: {EPOCHS}")
print(f"åºåˆ—é•¿åº¦: {SEQ_LENGTHS}")
print(f"Batch Size: {BATCH_SIZE}")
print(f"æ¨¡å‹: {MODELS_TO_TEST}")
print(f"å¿«é€Ÿæ¨¡å¼: {args.quick}")
print("=" * 80)


# ========================================
# æ¶ˆèå®éªŒçš„æ³¨æ„åŠ›å˜ä½“
# ========================================

class TimeDecayRichAttention(nn.Module):
    """æ—¶é—´è¡°å‡æ³¨æ„åŠ›"""
    
    def __init__(self, input_dim, hidden_dims=[64, 32], time_decay=0.1):
        super().__init__()
        self.time_decay = time_decay
        mlp_input = 4 * input_dim
        layers = []
        prev_dim = mlp_input
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.PReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        self.attention_mlp = nn.Sequential(*layers)
    
    def forward(self, query, keys, keys_mask=None):
        batch_size, seq_len, dim = keys.shape
        query_expanded = query.unsqueeze(1).expand(-1, seq_len, -1)
        attention_input = torch.cat([
            keys, query_expanded,
            keys * query_expanded,
            keys - query_expanded
        ], dim=-1)
        attention_scores = self.attention_mlp(attention_input).squeeze(-1)
        
        positions = torch.arange(seq_len, device=keys.device).float()
        time_weights = torch.exp(self.time_decay * (positions - seq_len + 1))
        attention_scores = attention_scores * time_weights.unsqueeze(0)
        
        if keys_mask is not None:
            attention_scores = attention_scores.masked_fill(~keys_mask.bool(), -1e9)
        attention_weights = F.softmax(attention_scores, dim=-1)
        weighted_sum = torch.sum(attention_weights.unsqueeze(-1) * keys, dim=1)
        return weighted_sum, attention_weights


class MultiHeadRichAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›"""
    
    def __init__(self, input_dim, num_heads=4, hidden_dims=[64, 32]):
        super().__init__()
        self.num_heads = num_heads
        self.attention_heads = nn.ModuleList([
            self._build_attention_mlp(4 * input_dim, hidden_dims)
            for _ in range(num_heads)
        ])
        self.output_proj = nn.Linear(input_dim, input_dim)
    
    def _build_attention_mlp(self, input_dim, hidden_dims):
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.PReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        return nn.Sequential(*layers)
    
    def forward(self, query, keys, keys_mask=None):
        batch_size, seq_len, dim = keys.shape
        query_expanded = query.unsqueeze(1).expand(-1, seq_len, -1)
        attention_input = torch.cat([
            keys, query_expanded,
            keys * query_expanded,
            keys - query_expanded
        ], dim=-1)
        
        head_outputs = []
        for head in self.attention_heads:
            scores = head(attention_input).squeeze(-1)
            if keys_mask is not None:
                scores = scores.masked_fill(~keys_mask.bool(), -1e9)
            weights = F.softmax(scores, dim=-1)
            output = torch.sum(weights.unsqueeze(-1) * keys, dim=1)
            head_outputs.append(output)
        
        combined = torch.stack(head_outputs, dim=1).mean(dim=1)
        return self.output_proj(combined), None


class DINRichVariant(nn.Module):
    """DIN æ¶ˆèå˜ä½“"""
    
    def __init__(self, num_items, num_users, feature_dims, embedding_dim=64,
                 attention_type='base', enhanced_mlp=False):
        super().__init__()
        self.attention_type = attention_type
        self.enhanced_mlp = enhanced_mlp
        
        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)
        self.user_embedding = nn.Embedding(num_users + 1, embedding_dim, padding_idx=0)
        
        self.feature_embeddings = nn.ModuleDict()
        for name, num_values in feature_dims.items():
            self.feature_embeddings[name] = nn.Embedding(num_values + 1, embedding_dim // 4, padding_idx=0)
        
        feature_embed_dim = (embedding_dim // 4) * len(feature_dims)
        self.total_embed_dim = embedding_dim + feature_embed_dim
        
        # é€‰æ‹©æ³¨æ„åŠ›ç±»å‹
        if attention_type == 'time_decay':
            self.attention = TimeDecayRichAttention(self.total_embed_dim)
        elif attention_type == 'multi_head':
            self.attention = MultiHeadRichAttention(self.total_embed_dim, num_heads=4)
        else:
            self.attention = AttentionLayer(self.total_embed_dim)
        
        # MLP
        mlp_input_dim = self.total_embed_dim * 3 + embedding_dim
        if enhanced_mlp:
            self.mlp = nn.Sequential(
                nn.Linear(mlp_input_dim, 256),
                nn.BatchNorm1d(256),
                nn.PReLU(),
                nn.Dropout(0.2),
                nn.Linear(256, 128),
                nn.BatchNorm1d(128),
                nn.PReLU(),
                nn.Dropout(0.2),
                nn.Linear(128, 64),
                nn.PReLU(),
                nn.Linear(64, 1)
            )
        else:
            self.mlp = nn.Sequential(
                nn.Linear(mlp_input_dim, 128),
                nn.PReLU(),
                nn.Linear(128, 64),
                nn.PReLU(),
                nn.Linear(64, 1)
            )
    
    def _get_rich_embedding(self, item_ids, features):
        item_emb = self.item_embedding(item_ids)
        feature_embs = []
        for name, emb_layer in self.feature_embeddings.items():
            if name in features:
                feature_embs.append(emb_layer(features[name]))
        if feature_embs:
            if len(item_emb.shape) == 2:
                feature_cat = torch.cat(feature_embs, dim=-1)
            else:
                feature_cat = torch.cat(feature_embs, dim=-1)
            return torch.cat([item_emb, feature_cat], dim=-1)
        return item_emb
    
    def forward(self, batch):
        item_seq = batch['item_seq']
        target_item = batch['target_item']
        user_id = batch['user_id']
        seq_mask = (item_seq > 0).float()
        
        seq_features = {k: v for k, v in batch.items() 
                       if k.endswith('_seq') and k != 'item_seq'}
        seq_emb = self._get_rich_embedding(item_seq, seq_features)
        
        target_features = {k.replace('target_', ''): v for k, v in batch.items() 
                          if k.startswith('target_') and k != 'target_item'}
        target_emb = self._get_rich_embedding(target_item, target_features)
        
        user_emb = self.user_embedding(user_id)
        
        interest_emb, _ = self.attention(target_emb, seq_emb, seq_mask)
        
        seq_mean = (seq_emb * seq_mask.unsqueeze(-1)).sum(dim=1) / (seq_mask.sum(dim=1, keepdim=True) + 1e-8)
        
        mlp_input = torch.cat([interest_emb, target_emb, seq_mean, user_emb], dim=-1)
        logits = self.mlp(mlp_input).squeeze(-1)
        return logits


# ========================================
# æ··åˆç²¾æ’æ¨¡å—
# ========================================

class HybridRanker:
    """DIN + LightGBM æ··åˆç²¾æ’"""
    
    def __init__(self, din_model, device='cpu'):
        self.din_model = din_model
        self.device = device
        self.lgb_model = None
    
    @torch.no_grad()
    def extract_din_features(self, data_loader):
        """æå– DIN åµŒå…¥ä½œä¸ºç‰¹å¾"""
        self.din_model.eval()
        self.din_model.to(self.device)
        
        all_embeddings = []
        all_scores = []
        all_labels = []
        
        for batch in data_loader:
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                     for k, v in batch.items()}
            
            # è·å–åµŒå…¥
            item_seq = batch['item_seq']
            seq_emb = self.din_model.item_embedding(item_seq)
            target_emb = self.din_model.item_embedding(batch['target_item'])
            user_emb = self.din_model.user_embedding(batch['user_id'])
            
            seq_mask = (item_seq > 0).float()
            seq_mean = (seq_emb * seq_mask.unsqueeze(-1)).sum(dim=1) / (seq_mask.sum(dim=1, keepdim=True) + 1e-8)
            
            # æ‹¼æ¥ç‰¹å¾
            features = torch.cat([target_emb, user_emb, seq_mean], dim=-1)
            all_embeddings.append(features.cpu().numpy())
            
            # DIN åˆ†æ•°
            score = torch.sigmoid(self.din_model(batch))
            all_scores.append(score.cpu().numpy())
            all_labels.append(batch['label'].cpu().numpy())
        
        embeddings = np.concatenate(all_embeddings, axis=0)
        scores = np.concatenate(all_scores, axis=0)
        labels = np.concatenate(all_labels, axis=0)
        
        # æ‹¼æ¥ DIN åˆ†æ•°ä½œä¸ºç‰¹å¾
        features = np.column_stack([embeddings, scores])
        return features, labels
    
    def train_lgb(self, train_loader, valid_loader):
        """è®­ç»ƒ LightGBM"""
        if not HAS_LIGHTGBM:
            return None
        
        X_train, y_train = self.extract_din_features(train_loader)
        X_valid, y_valid = self.extract_din_features(valid_loader)
        
        params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'verbose': -1,
            'random_state': 2020
        }
        
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_valid, label=y_valid)
        
        self.lgb_model = lgb.train(
            params, train_data,
            num_boost_round=300,
            valid_sets=[valid_data],
            callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)]
        )
        
        return self.lgb_model
    
    def evaluate(self, test_loader):
        """è¯„ä¼°æ··åˆæ¨¡å‹"""
        from sklearn.metrics import roc_auc_score, log_loss
        
        X_test, y_test = self.extract_din_features(test_loader)
        y_pred = self.lgb_model.predict(X_test)
        
        auc = roc_auc_score(y_test, y_pred)
        logloss = log_loss(y_test, y_pred)
        
        return {'auc': auc, 'logloss': logloss}


# ========================================
# å®éªŒä¸€ï¼šåºåˆ—é•¿åº¦æ•æ„Ÿæ€§ + æ¨¡å‹å¯¹æ¯”
# ========================================

def run_experiment1(dataset_name):
    """å®éªŒä¸€ï¼šä¸åŒåºåˆ—é•¿åº¦ä¸‹å„æ¨¡å‹çš„è¡¨ç°"""
    print("\n" + "=" * 80)
    print(f"ğŸ“Š å®éªŒä¸€ï¼šåºåˆ—é•¿åº¦æ•æ„Ÿæ€§ + æ¨¡å‹å¯¹æ¯” [{dataset_name}]")
    print("=" * 80)
    
    results = []
    
    for seq_length in SEQ_LENGTHS:
        print(f"\nğŸ”¬ åºåˆ—é•¿åº¦: {seq_length}")
        
        train_loader, valid_loader, test_loader, dataset_info, fp = get_rich_dataloaders(
            data_dir='./data',
            dataset_name=dataset_name,
            max_seq_length=seq_length,
            batch_size=BATCH_SIZE
        )
        
        for model_name in MODELS_TO_TEST:
            print(f"  ğŸš€ {model_name}...", end=" ", flush=True)
            
            try:
                if model_name == 'DIN':
                    model = DINRichLite(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM
                    )
                elif model_name == 'GRU4Rec':
                    model = GRU4Rec(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM,
                        hidden_dim=EMBEDDING_DIM
                    )
                elif model_name == 'SASRec':
                    model = SASRec(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM,
                        num_heads=2,
                        num_layers=2,
                        max_seq_len=seq_length
                    )
                elif model_name == 'NARM':
                    model = NARM(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM,
                        hidden_dim=EMBEDDING_DIM
                    )
                elif model_name == 'AvgPool':
                    model = SimpleAveragePoolingRich(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM
                    )
                
                trainer = RichTrainer(model=model, device=DEVICE)
                t1 = time.time()
                train_result = trainer.fit(
                    train_loader=train_loader,
                    valid_loader=valid_loader,
                    epochs=EPOCHS,
                    early_stopping_patience=10,
                    show_progress=False
                )
                train_time = time.time() - t1
                
                test_metrics = trainer.evaluate(test_loader)
                speed = measure_inference_speed_rich(model, test_loader, DEVICE)
                
                results.append({
                    'experiment': 'exp1_seq_model',
                    'dataset': dataset_name,
                    'seq_length': seq_length,
                    'model': model_name,
                    'test_auc': test_metrics['auc'],
                    'test_logloss': test_metrics['logloss'],
                    'best_valid_auc': train_result['best_valid_auc'],
                    'train_time_sec': train_time,
                    'qps': speed['qps'],
                    'num_params': sum(p.numel() for p in model.parameters()),
                    'status': 'success'
                })
                
                print(f"AUC={test_metrics['auc']:.4f}, Time={train_time:.1f}s")
                
            except Exception as e:
                print(f"âŒ {str(e)[:50]}")
                results.append({
                    'experiment': 'exp1_seq_model',
                    'dataset': dataset_name,
                    'seq_length': seq_length,
                    'model': model_name,
                    'test_auc': None,
                    'status': f'error: {str(e)[:100]}'
                })
    
    return results


# ========================================
# å®éªŒäºŒï¼šæ–¹æ³•å¯¹æ¯” + LightGBM + æ··åˆç²¾æ’
# ========================================

def run_experiment2(dataset_name):
    """å®éªŒäºŒï¼šDIN vs ä¼ ç»Ÿæ–¹æ³• + æ··åˆç²¾æ’"""
    print("\n" + "=" * 80)
    print(f"ğŸ“Š å®éªŒäºŒï¼šæ–¹æ³•å¯¹æ¯” + æ··åˆç²¾æ’ [{dataset_name}]")
    print("=" * 80)
    
    results = []
    seq_length = 50
    
    train_loader, valid_loader, test_loader, dataset_info, fp = get_rich_dataloaders(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        batch_size=BATCH_SIZE
    )
    
    din_model = None  # ä¿å­˜ç”¨äºæ··åˆç²¾æ’
    
    # æµ‹è¯•å„æ·±åº¦æ¨¡å‹
    for model_name in MODELS_TO_TEST:
        print(f"  ğŸš€ {model_name}...", end=" ", flush=True)
        
        try:
            if model_name == 'DIN':
                model = DINRichLite(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM
                )
                din_model = model  # ä¿å­˜
            elif model_name == 'GRU4Rec':
                model = GRU4Rec(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM,
                    hidden_dim=EMBEDDING_DIM
                )
            elif model_name == 'SASRec':
                model = SASRec(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM,
                    num_heads=2,
                    num_layers=2,
                    max_seq_len=seq_length
                )
            elif model_name == 'NARM':
                model = NARM(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM,
                    hidden_dim=EMBEDDING_DIM
                )
            elif model_name == 'AvgPool':
                model = SimpleAveragePoolingRich(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM
                )
            
            trainer = RichTrainer(model=model, device=DEVICE)
            t1 = time.time()
            train_result = trainer.fit(
                train_loader=train_loader,
                valid_loader=valid_loader,
                epochs=EPOCHS,
                early_stopping_patience=10,
                show_progress=False
            )
            train_time = time.time() - t1
            
            test_metrics = trainer.evaluate(test_loader)
            speed = measure_inference_speed_rich(model, test_loader, DEVICE)
            
            results.append({
                'experiment': 'exp2_method_compare',
                'dataset': dataset_name,
                'model': model_name,
                'test_auc': test_metrics['auc'],
                'test_logloss': test_metrics['logloss'],
                'train_time_sec': train_time,
                'qps': speed['qps'],
                'status': 'success'
            })
            
            print(f"AUC={test_metrics['auc']:.4f}")
            
        except Exception as e:
            print(f"âŒ {str(e)[:50]}")
            results.append({
                'experiment': 'exp2_method_compare',
                'dataset': dataset_name,
                'model': model_name,
                'test_auc': None,
                'status': f'error: {str(e)[:100]}'
            })
    
    # LightGBM å•ç‹¬
    if HAS_LIGHTGBM:
        print("  ğŸš€ LightGBM (pure)...", end=" ", flush=True)
        try:
            from sklearn.metrics import roc_auc_score, log_loss
            from sklearn.model_selection import train_test_split
            
            data_path = os.path.join('./data', dataset_name)
            if dataset_name == 'ml-100k':
                interactions = pd.read_csv(
                    os.path.join(data_path, 'u.data'),
                    sep='\t', names=['user_id', 'item_id', 'rating', 'timestamp']
                )
            else:
                interactions = pd.read_csv(
                    os.path.join(data_path, 'ratings.dat'),
                    sep='::', names=['user_id', 'item_id', 'rating', 'timestamp'],
                    engine='python'
                )
            
            interaction_extractor = InteractionFeatureExtractor(interactions)
            X, y, feature_names = prepare_lightgbm_features(
                interactions, fp, interaction_extractor, max_seq_length=seq_length
            )
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2020)
            X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.125, random_state=2020)
            
            params = {
                'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',
                'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8,
                'verbose': -1, 'random_state': 2020
            }
            
            train_data = lgb.Dataset(X_train, label=y_train)
            valid_data = lgb.Dataset(X_valid, label=y_valid)
            
            t1 = time.time()
            lgb_model = lgb.train(
                params, train_data, num_boost_round=500,
                valid_sets=[valid_data],
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )
            train_time = time.time() - t1
            
            y_pred = lgb_model.predict(X_test)
            test_auc = roc_auc_score(y_test, y_pred)
            
            results.append({
                'experiment': 'exp2_method_compare',
                'dataset': dataset_name,
                'model': 'LightGBM',
                'test_auc': test_auc,
                'train_time_sec': train_time,
                'status': 'success'
            })
            print(f"AUC={test_auc:.4f}")
            
        except Exception as e:
            print(f"âŒ {str(e)[:50]}")
            results.append({
                'experiment': 'exp2_method_compare',
                'dataset': dataset_name,
                'model': 'LightGBM',
                'test_auc': None,
                'status': f'error: {str(e)[:100]}'
            })
    
    # æ··åˆç²¾æ’ï¼šDIN + LightGBM
    if HAS_LIGHTGBM and din_model is not None:
        print("  ğŸš€ Hybrid (DIN + LightGBM)...", end=" ", flush=True)
        try:
            hybrid = HybridRanker(din_model, device=DEVICE)
            t1 = time.time()
            hybrid.train_lgb(train_loader, valid_loader)
            train_time = time.time() - t1
            
            test_metrics = hybrid.evaluate(test_loader)
            
            results.append({
                'experiment': 'exp2_hybrid',
                'dataset': dataset_name,
                'model': 'DIN+LightGBM',
                'test_auc': test_metrics['auc'],
                'test_logloss': test_metrics['logloss'],
                'train_time_sec': train_time,
                'status': 'success'
            })
            print(f"AUC={test_metrics['auc']:.4f}")
            
        except Exception as e:
            print(f"âŒ {str(e)[:50]}")
            results.append({
                'experiment': 'exp2_hybrid',
                'dataset': dataset_name,
                'model': 'DIN+LightGBM',
                'test_auc': None,
                'status': f'error: {str(e)[:100]}'
            })
    
    return results


# ========================================
# å®éªŒä¸‰ï¼šDIN æ¶ˆèå®éªŒ
# ========================================

def run_experiment3(dataset_name):
    """å®éªŒä¸‰ï¼šDIN æ”¹è¿›æ¶ˆèå®éªŒ"""
    print("\n" + "=" * 80)
    print(f"ğŸ“Š å®éªŒä¸‰ï¼šDIN æ¶ˆèå®éªŒ [{dataset_name}]")
    print("=" * 80)
    
    results = []
    seq_length = 50
    
    train_loader, valid_loader, test_loader, dataset_info, fp = get_rich_dataloaders(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        batch_size=BATCH_SIZE
    )
    
    # æ¶ˆèå˜ä½“
    ablation_variants = [
        ('DIN-Base', 'base', False),
        ('DIN-TimeDec', 'time_decay', False),
        ('DIN-MultiHead', 'multi_head', False),
        ('DIN-Enhanced', 'base', True),
        ('DIN-Full', 'time_decay', True),
    ]
    
    for variant_name, attention_type, enhanced_mlp in ablation_variants:
        print(f"  ğŸš€ {variant_name}...", end=" ", flush=True)
        
        try:
            model = DINRichVariant(
                num_items=dataset_info['num_items'],
                num_users=dataset_info['num_users'],
                feature_dims=dataset_info['feature_dims'],
                embedding_dim=EMBEDDING_DIM,
                attention_type=attention_type,
                enhanced_mlp=enhanced_mlp
            )
            
            trainer = RichTrainer(model=model, device=DEVICE)
            t1 = time.time()
            train_result = trainer.fit(
                train_loader=train_loader,
                valid_loader=valid_loader,
                epochs=EPOCHS,
                early_stopping_patience=10,
                show_progress=False
            )
            train_time = time.time() - t1
            
            test_metrics = trainer.evaluate(test_loader)
            speed = measure_inference_speed_rich(model, test_loader, DEVICE)
            
            results.append({
                'experiment': 'exp3_ablation',
                'dataset': dataset_name,
                'variant': variant_name,
                'attention_type': attention_type,
                'enhanced_mlp': enhanced_mlp,
                'test_auc': test_metrics['auc'],
                'test_logloss': test_metrics['logloss'],
                'best_valid_auc': train_result['best_valid_auc'],
                'train_time_sec': train_time,
                'qps': speed['qps'],
                'num_params': sum(p.numel() for p in model.parameters()),
                'status': 'success'
            })
            
            print(f"AUC={test_metrics['auc']:.4f}")
            
        except Exception as e:
            print(f"âŒ {str(e)[:50]}")
            results.append({
                'experiment': 'exp3_ablation',
                'dataset': dataset_name,
                'variant': variant_name,
                'test_auc': None,
                'status': f'error: {str(e)[:100]}'
            })
    
    return results


# ========================================
# ä¸»ç¨‹åº
# ========================================

if __name__ == '__main__':
    all_results = []
    experiment_start = datetime.now()
    
    print(f"\nâ° å®éªŒå¼€å§‹æ—¶é—´: {experiment_start.strftime('%Y-%m-%d %H:%M:%S')}")
    
    for dataset in DATASETS:
        print(f"\n{'='*80}")
        print(f"ğŸ“ æ•°æ®é›†: {dataset.upper()}")
        print(f"{'='*80}")
        
        if 1 in EXPERIMENTS_TO_RUN:
            results1 = run_experiment1(dataset)
            all_results.extend(results1)
        
        if 2 in EXPERIMENTS_TO_RUN:
            results2 = run_experiment2(dataset)
            all_results.extend(results2)
        
        if 3 in EXPERIMENTS_TO_RUN:
            results3 = run_experiment3(dataset)
            all_results.extend(results3)
    
    # ä¿å­˜ç»“æœ
    experiment_end = datetime.now()
    total_time = (experiment_end - experiment_start).total_seconds()
    
    df_results = pd.DataFrame(all_results)
    timestamp = experiment_start.strftime('%Y%m%d_%H%M%S')
    
    # CSV
    csv_file = os.path.join(RESULTS_DIR, f'all_results_{timestamp}.csv')
    df_results.to_csv(csv_file, index=False)
    
    # JSON æŠ¥å‘Š
    report = {
        'timestamp': timestamp,
        'device': DEVICE,
        'gpu_name': torch.cuda.get_device_name(0) if DEVICE == 'cuda' else 'CPU',
        'datasets': DATASETS,
        'experiments': EXPERIMENTS_TO_RUN,
        'epochs': EPOCHS,
        'seq_lengths': SEQ_LENGTHS,
        'models': MODELS_TO_TEST,
        'total_time_minutes': total_time / 60,
        'num_results': len(all_results),
        'results': all_results
    }
    
    json_file = os.path.join(RESULTS_DIR, f'report_{timestamp}.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ“‹ å®éªŒå®Œæˆï¼")
    print("=" * 80)
    print(f"æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"å®éªŒæ•°é‡: {len(all_results)}")
    print(f"\nğŸ“‚ ç»“æœæ–‡ä»¶:")
    print(f"   {csv_file}")
    print(f"   {json_file}")
    
    # å„æ•°æ®é›†æœ€ä½³ç»“æœ
    print("\nğŸ“Š å„å®éªŒæœ€ä½³ AUC:")
    df_success = df_results[df_results['status'] == 'success']
    
    for exp_name in df_success['experiment'].unique():
        df_exp = df_success[df_success['experiment'] == exp_name]
        if len(df_exp) > 0 and 'test_auc' in df_exp.columns:
            best = df_exp.loc[df_exp['test_auc'].idxmax()]
            model_col = 'model' if 'model' in best else 'variant'
            print(f"  {exp_name}: {best.get(model_col, 'N/A')} = {best['test_auc']:.4f}")
    
    print("=" * 80)
    print("âœ… æ‰€æœ‰å®éªŒå®Œæˆï¼")
