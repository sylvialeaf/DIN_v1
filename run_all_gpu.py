#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
äº‘ç«¯ GPU å®Œæ•´å®žéªŒè„šæœ¬

é€‚åˆåœ¨ AutoDL / Colab / é˜¿é‡Œäº‘ç­‰ GPU çŽ¯å¢ƒè¿è¡Œã€‚
æ”¯æŒ ml-100k å’Œ ml-1m åŒæ•°æ®é›†ã€‚

ä½¿ç”¨æ–¹æ³•:
    python run_all_gpu.py                    # è¿è¡Œæ‰€æœ‰å®žéªŒï¼ˆä¸¤ä¸ªæ•°æ®é›†ï¼‰
    python run_all_gpu.py --dataset ml-100k  # åªè¿è¡Œ ml-100k
    python run_all_gpu.py --dataset ml-1m    # åªè¿è¡Œ ml-1m
    python run_all_gpu.py --quick            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼

é¢„ä¼°æ—¶é—´ (GPU):
    ml-100k: çº¦ 15-20 åˆ†é’Ÿ
    ml-1m:   çº¦ 60-90 åˆ†é’Ÿ
    æ€»è®¡:    çº¦ 1.5-2 å°æ—¶
"""

import os
import sys
import argparse
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import json
import time

from data_loader import get_rich_dataloaders
from models import DINRichLite, SimpleAveragePoolingRich, GRU4Rec, SASRec, NARM
from trainer import RichTrainer, measure_inference_speed_rich
from feature_engineering import FeatureProcessor, InteractionFeatureExtractor, prepare_lightgbm_features

# ========================================
# é…ç½®
# ========================================

# è§£æžå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description='äº‘ç«¯ GPU å®Œæ•´å®žéªŒ')
parser.add_argument('--dataset', type=str, default='both', 
                    choices=['ml-100k', 'ml-1m', 'both'],
                    help='æ•°æ®é›†é€‰æ‹©')
parser.add_argument('--quick', action='store_true', 
                    help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå‡å°‘ epochs å’Œåºåˆ—é•¿åº¦ï¼‰')
parser.add_argument('--epochs', type=int, default=50,
                    help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 50ï¼‰')
args = parser.parse_args()

# è®¾å¤‡æ£€æµ‹
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# å®žéªŒå‚æ•°
if args.quick:
    EPOCHS = 10
    SEQ_LENGTHS = [20, 50]
    BATCH_SIZE = 512
else:
    EPOCHS = args.epochs
    SEQ_LENGTHS = [20, 50, 100, 150]
    BATCH_SIZE = 512 if DEVICE == 'cuda' else 256

EMBEDDING_DIM = 64
MODELS_TO_TEST = ['DIN', 'GRU4Rec', 'SASRec', 'NARM', 'AvgPool']

# æ•°æ®é›†
if args.dataset == 'both':
    DATASETS = ['ml-100k', 'ml-1m']
else:
    DATASETS = [args.dataset]

# ç»“æžœç›®å½•
RESULTS_DIR = os.path.join(os.path.dirname(__file__), 'results_gpu')
os.makedirs(RESULTS_DIR, exist_ok=True)

print("=" * 80)
print("ðŸš€ äº‘ç«¯ GPU å®Œæ•´å®žéªŒ")
print("=" * 80)
print(f"è®¾å¤‡: {DEVICE}")
print(f"æ•°æ®é›†: {DATASETS}")
print(f"Epochs: {EPOCHS}")
print(f"åºåˆ—é•¿åº¦: {SEQ_LENGTHS}")
print(f"Batch Size: {BATCH_SIZE}")
print(f"æ¨¡åž‹: {MODELS_TO_TEST}")
print(f"å¿«é€Ÿæ¨¡å¼: {args.quick}")
print("=" * 80)

all_results = []
experiment_start = datetime.now()


# ========================================
# å®žéªŒä¸€ï¼šåºåˆ—é•¿åº¦æ•æ„Ÿæ€§ + æ¨¡åž‹å¯¹æ¯”
# ========================================

def run_experiment1(dataset_name):
    """å®žéªŒä¸€ï¼šä¸åŒåºåˆ—é•¿åº¦ä¸‹å„æ¨¡åž‹çš„è¡¨çŽ°"""
    print("\n" + "=" * 80)
    print(f"ðŸ“Š å®žéªŒä¸€ï¼šåºåˆ—é•¿åº¦æ•æ„Ÿæ€§ [{dataset_name}]")
    print("=" * 80)
    
    results = []
    
    for seq_length in SEQ_LENGTHS:
        print(f"\nðŸ”¬ åºåˆ—é•¿åº¦: {seq_length}")
        
        # åŠ è½½æ•°æ®
        train_loader, valid_loader, test_loader, dataset_info, fp = get_rich_dataloaders(
            data_dir='./data',
            dataset_name=dataset_name,
            max_seq_length=seq_length,
            batch_size=BATCH_SIZE
        )
        
        for model_name in MODELS_TO_TEST:
            print(f"  ðŸš€ {model_name}...", end=" ", flush=True)
            
            try:
                # åˆ›å»ºæ¨¡åž‹
                if model_name == 'DIN':
                    model = DINRichLite(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM
                    )
                elif model_name == 'GRU4Rec':
                    model = GRU4Rec(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM,
                        hidden_dim=EMBEDDING_DIM
                    )
                elif model_name == 'SASRec':
                    model = SASRec(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM,
                        num_heads=2,
                        num_layers=2,
                        max_seq_len=seq_length
                    )
                elif model_name == 'NARM':
                    model = NARM(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM,
                        hidden_dim=EMBEDDING_DIM
                    )
                elif model_name == 'AvgPool':
                    model = SimpleAveragePoolingRich(
                        num_items=dataset_info['num_items'],
                        num_users=dataset_info['num_users'],
                        feature_dims=dataset_info['feature_dims'],
                        embedding_dim=EMBEDDING_DIM
                    )
                
                # è®­ç»ƒ
                trainer = RichTrainer(model=model, device=DEVICE)
                t1 = time.time()
                train_result = trainer.fit(
                    train_loader=train_loader,
                    valid_loader=valid_loader,
                    epochs=EPOCHS,
                    early_stopping_patience=10,
                    show_progress=False
                )
                train_time = time.time() - t1
                
                # è¯„ä¼°
                test_metrics = trainer.evaluate(test_loader)
                speed = measure_inference_speed_rich(model, test_loader, DEVICE)
                
                results.append({
                    'experiment': 'exp1',
                    'dataset': dataset_name,
                    'seq_length': seq_length,
                    'model': model_name,
                    'test_auc': test_metrics['auc'],
                    'test_logloss': test_metrics['logloss'],
                    'best_valid_auc': train_result['best_valid_auc'],
                    'train_time_sec': train_time,
                    'qps': speed['qps'],
                    'num_params': sum(p.numel() for p in model.parameters()),
                    'status': 'success'
                })
                
                print(f"AUC={test_metrics['auc']:.4f}, Time={train_time:.1f}s")
                
            except Exception as e:
                print(f"âŒ {str(e)[:50]}")
                results.append({
                    'experiment': 'exp1',
                    'dataset': dataset_name,
                    'seq_length': seq_length,
                    'model': model_name,
                    'test_auc': None,
                    'status': f'error: {str(e)[:100]}'
                })
    
    return results


# ========================================
# å®žéªŒäºŒï¼šæ–¹æ³•å¯¹æ¯” + LightGBM
# ========================================

def run_experiment2(dataset_name):
    """å®žéªŒäºŒï¼šDIN vs ä¼ ç»Ÿæ–¹æ³•"""
    print("\n" + "=" * 80)
    print(f"ðŸ“Š å®žéªŒäºŒï¼šæ–¹æ³•å¯¹æ¯” [{dataset_name}]")
    print("=" * 80)
    
    results = []
    seq_length = 50
    
    # åŠ è½½æ•°æ®
    train_loader, valid_loader, test_loader, dataset_info, fp = get_rich_dataloaders(
        data_dir='./data',
        dataset_name=dataset_name,
        max_seq_length=seq_length,
        batch_size=BATCH_SIZE
    )
    
    # æµ‹è¯•å„æ¨¡åž‹
    for model_name in MODELS_TO_TEST:
        print(f"  ðŸš€ {model_name}...", end=" ", flush=True)
        
        try:
            if model_name == 'DIN':
                model = DINRichLite(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM
                )
            elif model_name == 'GRU4Rec':
                model = GRU4Rec(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM,
                    hidden_dim=EMBEDDING_DIM
                )
            elif model_name == 'SASRec':
                model = SASRec(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM,
                    num_heads=2,
                    num_layers=2,
                    max_seq_len=seq_length
                )
            elif model_name == 'NARM':
                model = NARM(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM,
                    hidden_dim=EMBEDDING_DIM
                )
            elif model_name == 'AvgPool':
                model = SimpleAveragePoolingRich(
                    num_items=dataset_info['num_items'],
                    num_users=dataset_info['num_users'],
                    feature_dims=dataset_info['feature_dims'],
                    embedding_dim=EMBEDDING_DIM
                )
            
            trainer = RichTrainer(model=model, device=DEVICE)
            t1 = time.time()
            train_result = trainer.fit(
                train_loader=train_loader,
                valid_loader=valid_loader,
                epochs=EPOCHS,
                early_stopping_patience=10,
                show_progress=False
            )
            train_time = time.time() - t1
            
            test_metrics = trainer.evaluate(test_loader)
            speed = measure_inference_speed_rich(model, test_loader, DEVICE)
            
            results.append({
                'experiment': 'exp2',
                'dataset': dataset_name,
                'model': model_name,
                'test_auc': test_metrics['auc'],
                'test_logloss': test_metrics['logloss'],
                'train_time_sec': train_time,
                'qps': speed['qps'],
                'status': 'success'
            })
            
            print(f"AUC={test_metrics['auc']:.4f}")
            
        except Exception as e:
            print(f"âŒ {str(e)[:50]}")
            results.append({
                'experiment': 'exp2',
                'dataset': dataset_name,
                'model': model_name,
                'test_auc': None,
                'status': f'error: {str(e)[:100]}'
            })
    
    # LightGBM
    print("  ðŸš€ LightGBM...", end=" ", flush=True)
    try:
        import lightgbm as lgb
        from sklearn.metrics import roc_auc_score, log_loss
        from sklearn.model_selection import train_test_split
        
        data_path = os.path.join('./data', dataset_name)
        if dataset_name == 'ml-100k':
            interactions = pd.read_csv(
                os.path.join(data_path, 'u.data'),
                sep='\t', names=['user_id', 'item_id', 'rating', 'timestamp']
            )
        else:
            interactions = pd.read_csv(
                os.path.join(data_path, 'ratings.dat'),
                sep='::', names=['user_id', 'item_id', 'rating', 'timestamp'],
                engine='python'
            )
        
        interaction_extractor = InteractionFeatureExtractor(interactions)
        X, y, feature_names = prepare_lightgbm_features(
            interactions, fp, interaction_extractor, max_seq_length=seq_length
        )
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2020)
        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.125, random_state=2020)
        
        params = {
            'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',
            'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.8,
            'verbose': -1, 'random_state': 2020
        }
        
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_valid, label=y_valid)
        
        t1 = time.time()
        lgb_model = lgb.train(
            params, train_data, num_boost_round=500,
            valid_sets=[valid_data],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        train_time = time.time() - t1
        
        y_pred = lgb_model.predict(X_test)
        test_auc = roc_auc_score(y_test, y_pred)
        
        results.append({
            'experiment': 'exp2',
            'dataset': dataset_name,
            'model': 'LightGBM',
            'test_auc': test_auc,
            'train_time_sec': train_time,
            'status': 'success'
        })
        print(f"AUC={test_auc:.4f}")
        
    except Exception as e:
        print(f"âŒ {str(e)[:50]}")
        results.append({
            'experiment': 'exp2',
            'dataset': dataset_name,
            'model': 'LightGBM',
            'test_auc': None,
            'status': f'error: {str(e)[:100]}'
        })
    
    return results


# ========================================
# ä¸»ç¨‹åº
# ========================================

if __name__ == '__main__':
    print(f"\nâ° å®žéªŒå¼€å§‹æ—¶é—´: {experiment_start.strftime('%Y-%m-%d %H:%M:%S')}")
    
    for dataset in DATASETS:
        print(f"\n{'='*80}")
        print(f"ðŸ“ æ•°æ®é›†: {dataset.upper()}")
        print(f"{'='*80}")
        
        # è¿è¡Œå®žéªŒä¸€
        results1 = run_experiment1(dataset)
        all_results.extend(results1)
        
        # è¿è¡Œå®žéªŒäºŒ
        results2 = run_experiment2(dataset)
        all_results.extend(results2)
    
    # ä¿å­˜ç»“æžœ
    experiment_end = datetime.now()
    total_time = (experiment_end - experiment_start).total_seconds()
    
    df_results = pd.DataFrame(all_results)
    timestamp = experiment_start.strftime('%Y%m%d_%H%M%S')
    
    # CSV
    csv_file = os.path.join(RESULTS_DIR, f'all_results_{timestamp}.csv')
    df_results.to_csv(csv_file, index=False)
    
    # JSON æŠ¥å‘Š
    report = {
        'timestamp': timestamp,
        'device': DEVICE,
        'datasets': DATASETS,
        'epochs': EPOCHS,
        'seq_lengths': SEQ_LENGTHS,
        'models': MODELS_TO_TEST,
        'total_time_minutes': total_time / 60,
        'results': all_results
    }
    
    json_file = os.path.join(RESULTS_DIR, f'report_{timestamp}.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ðŸ“‹ å®žéªŒå®Œæˆï¼")
    print("=" * 80)
    print(f"æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"\nç»“æžœæ–‡ä»¶:")
    print(f"  - {csv_file}")
    print(f"  - {json_file}")
    
    # å„æ•°æ®é›†æœ€ä½³ç»“æžœ
    print("\nðŸ“Š å„æ•°æ®é›†æœ€ä½³ AUC:")
    df_success = df_results[df_results['status'] == 'success']
    for dataset in DATASETS:
        df_ds = df_success[df_success['dataset'] == dataset]
        if len(df_ds) > 0:
            best = df_ds.loc[df_ds['test_auc'].idxmax()]
            print(f"  {dataset}: {best['model']} = {best['test_auc']:.4f}")
    
    print("=" * 80)
